{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwNr0BSsC5xp"
   },
   "source": [
    "# 使用Seq2Seq进行机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "* [1. 文本数据预处理](#1)\n",
    "* [2. 创建批量数据](#2)\n",
    "* [3. 构建含注意力机制的Seq2Seq模型](#3)\n",
    "    * [3.1 构造编码器](#3.1)\n",
    "    * [3.2 使用Bahdanau Attention机制](#3.2)\n",
    "    * [3.3 构造解码器](#3.3)\n",
    "* [4. 训练模型](#4)\n",
    "    * [4.1 定义优化方法和损失函数](#4.1)\n",
    "    * [4.2 定义训练步骤](#4.2)\n",
    "    * [4.3 训练模型](#4.3)\n",
    "* [5. 翻译句子](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWb9hDOaC5xr"
   },
   "source": [
    "在这里将使用keras框架来搭建一个基于Attention的机器翻译模型。 本案例数据（原始下载路径）包含了19577条中文-英文的翻译句子对。 每一行代表了一个翻译对，并使用制表符tab作为中文（繁体字）和英文的分割。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ao13Nmz4C5xr"
   },
   "source": [
    "# 1. 文本数据预处理\n",
    "<a id=1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3847,
     "status": "ok",
     "timestamp": 1597644894018,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "h5cAn_CDC5xs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2046,
     "status": "ok",
     "timestamp": 1597644897639,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "tYlkzMXUC5xy",
    "outputId": "e547c829-c8b2-4d26-e211-6dfe34d9b86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本中一共有19578对文本。\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10000  #选取num_samples个样本进行训练\n",
    "\n",
    "# 数据路径\n",
    "data_path = './eng-chi.txt'\n",
    "\n",
    "# 读取数据\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "print(\"文本中一共有%s对文本。\"%(len(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKg7S4L4C5x2"
   },
   "source": [
    "因为在英文中需要需要保留英文符号，而分词是按照空格分词的，所以需要在分词前把标点符号前加一个空格。同时还要添加标志序列开始和结束的字符，中文使用jieba库分词，然后使用空格隔开，所以创建如下函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1817,
     "status": "ok",
     "timestamp": 1597644908362,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "F0T3diTfC5x4"
   },
   "outputs": [],
   "source": [
    "#处理英文句子函数\n",
    "def preprocess_engtext(w):\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格，在分词的时候可以把标点符号也考虑进去\n",
    "    # 例如： \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)#ps：很神奇\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\"，\"\"'\")，将所有字符替换为空格\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,']+\", \" \", w)\n",
    "    #删除结尾和开头的空格\n",
    "    w = w.strip()\n",
    "    # 给句子加上开始和结束标记\n",
    "    # 以便模型知道何时开始和结束预测\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "#同样的处理中文句子函数\n",
    "def preprocess_chitext(w):\n",
    "    #清除多余空格\n",
    "    w = re.sub(r'[\" \"]+', \"\", w)\n",
    "    #使用jieba库分词\n",
    "    w = \" \".join(jieba.lcut(w))\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2413,
     "status": "ok",
     "timestamp": 1597644913886,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "R6muvMf4C5x8",
    "outputId": "909c4f5a-4f2e-46cd-b9d3-62c7a0ada7ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.771 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际输入文本的句子的数量： 10000\n",
      "输入的最大文本长度： 12\n",
      "输出最大文本的长度： 15\n"
     ]
    }
   ],
   "source": [
    "# 储存文本，输入样本为英文，输出样本为中文\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "#从lines中取num_samples个样本,并进行预处理\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:    \n",
    "    input_text, target_text = line.split('\\t')\n",
    "    #对中英文样本进行预处理\n",
    "    input_text = preprocess_engtext(input_text)\n",
    "    target_text = preprocess_chitext(target_text)\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "# 统计英文文本的最大序列长度。\n",
    "max_encoder_seq_length = max([len(txt.split()) for txt in input_texts])\n",
    "# 统计中文文本的最大序列长度。\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
    "\n",
    "print('实际输入文本的句子的数量：', len(input_texts))\n",
    "print('输入的最大文本长度：', max_encoder_seq_length)\n",
    "print('输出最大文本的长度：', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEVKrgdbC5yC"
   },
   "source": [
    "对句子进行补齐可以选用各自最大的文本长度。下面查看一下得到的样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2971,
     "status": "ok",
     "timestamp": 1597644923252,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "Yg0YSd70C5yD",
    "outputId": "4609210c-8a11-4b1c-d4d0-835e86bc755a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> I agree with you on this issue . <end>\n",
      "<start> 我 同意 你 對 這問題 的 看法 。 <end>\n"
     ]
    }
   ],
   "source": [
    "print(input_texts[-1])\n",
    "print(target_texts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om2JUxrhC5yG"
   },
   "source": [
    "下面对文本进行分词处理，使用keras的Tokenizer方法创建函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3733,
     "status": "ok",
     "timestamp": 1597644930030,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "hmqZesPpC5yG"
   },
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    tokenizer = Tokenizer(filters='')#不要把标点符号也给过滤掉了\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    tensor = tokenizer.texts_to_sequences(texts)\n",
    "    tensor = pad_sequences(tensor, padding='post') #这里都选择往后填充，因为后面会用到将输入数据逆序\n",
    "    #返回处理好的序列和字典\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3828,
     "status": "ok",
     "timestamp": 1597644931502,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "Tu0E2p9IC5yK"
   },
   "outputs": [],
   "source": [
    "input_data, input_tokenizer = tokenize(input_texts)\n",
    "target_data, target_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhANS21fC5yN"
   },
   "source": [
    "得到两个文本的整数序列索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2535,
     "status": "ok",
     "timestamp": 1597644932529,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "d-PVjvjrC5yP",
    "outputId": "53378c52-f6b9-4cd4-cea3-55c9bc976f48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,    4,  358,   49,    5,   35,   19, 3566,    3,    2,    0,\n",
       "           0], dtype=int32),\n",
       " array([   1,    4,  220,    6,   73, 7081,    5,  748,    3,    2,    0,\n",
       "           0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[-1],target_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2473,
     "status": "ok",
     "timestamp": 1597644935505,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "VC9ex2-PC5yS",
    "outputId": "69b44de1-e8c7-4d74-bf30-55e78a48c167"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 12), (10000, 15))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape, target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rx5dzZCgC5yV"
   },
   "source": [
    "# 2. 创建批量数据\n",
    "<a id=2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6290,
     "status": "ok",
     "timestamp": 1597645066311,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "0gU7XUUGC5ya"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64   #生成批量数据集的批次大小\n",
    "steps_per_epoch = len(input_data)//BATCH_SIZE\n",
    "embedding_dim = 100 #词嵌入维度\n",
    "units = 512   #GRU隐层神经元个数\n",
    "input_dim = len(input_tokenizer.word_index) + 1    #英文字典的大小\n",
    "target_dim = len(target_tokenizer.word_index) + 1  #中文字典的大小\n",
    "#生成一个产生批量数据的迭代器\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data)).shuffle(len(input_data))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ys0NdHopC5yd"
   },
   "source": [
    "比如从迭代器中生成一个`BATCH_SIZE`大小的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2116,
     "status": "ok",
     "timestamp": 1597645073116,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "-eV9z6u7C5yd",
    "outputId": "9aeb8601-366b-4252-db4d-e94c5bdbafb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 12]), TensorShape([64, 15]))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zs5r0b5GC5yi"
   },
   "source": [
    "# 3. 构建含注意力机制的Seq2Seq模型\n",
    "<a id=3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yq03OR1mC5yi"
   },
   "source": [
    "## 3.1 构造编码器\n",
    "<a id=3.1></a>\n",
    "\n",
    "编码器使用GRU对文本进行编码，最后的一个输出作为解码器的输入，但同时也要记录每个时间步的输出，因为注意力机制生成的向量实际上就是这些输出的加权和。因此GRU层的参数中`return_sequences=True`可以得到每个时间步的输出，`return_state=True`会得到最后一个时间步的输出。本案例的模型构建部分使用的是tensorflow官方教程里的模型：https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3426,
     "status": "ok",
     "timestamp": 1597645087041,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "g6DbfzvRC5yj"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_dim, embedding_dim, enc_units, batch_sz):\n",
    "        \"\"\"\n",
    "        vocab_dim: 输入英文数据的维度（也就是字典大小）\n",
    "        embedding_dim: 词嵌入后的维度\n",
    "        enc_units: 编码器隐层神经元数量\n",
    "        batch_sz: 输入数据的样本数，这里用于初始化GRU的初始状态\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_dim, embedding_dim)\n",
    "        self.gru = GRU(self.enc_units,\n",
    "                        return_sequences=True,\n",
    "                        return_state=True,\n",
    "                        recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x : 输入数据，2维张量（批量大小，序列长度），因为有embedding层\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x,initial_state = hidden)\n",
    "        return output, state   \n",
    "        # output 为每个时间步的输出，是个3维张量(批量大小, 序列长度（时间步）, 神经元数量)，\n",
    "        # state为最后一个时间步的输出（向量），是2维张量(批量大小, 神经元数量) \n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        #初始化开始的隐层状态\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nw0Dbh9OC5ym"
   },
   "source": [
    "使用上次创建好的数据样例进行初步测试，检查输出的维度是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6730,
     "status": "ok",
     "timestamp": 1597645092910,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "KF-z2f--C5yn",
    "outputId": "dbd9b60d-f8eb-4aac-d3ae-bc0021ba278d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码器的第一个输出维度: (64, 12, 512) (批量大小, 序列长度（时间步）, 神经元数量) \n",
      "编码器的第二个输出维度: (64, 512)(批量大小, 神经元数量) \n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim, embedding_dim, units, BATCH_SIZE)\n",
    "# 样本输入\n",
    "sample_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_state = encoder(example_input_batch, sample_state)\n",
    "print ('编码器的第一个输出维度: {} (批量大小, 序列长度（时间步）, 神经元数量) '.format(sample_output.shape))\n",
    "print ('编码器的第二个输出维度: {}(批量大小, 神经元数量) '.format(sample_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOTiwr1zC5yq"
   },
   "source": [
    "## 3.2 使用Bahdanau Attention机制\n",
    "<a id=3.2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9ff5SUJC5yq"
   },
   "source": [
    "先简要介绍一下Bahdanau Attention机制的公式，设以解码器在时间步$t'$的隐藏状态$s_{t'}$与编码器在时间步$t$的隐藏状态$h_t$为输入，得到权重：\n",
    "\n",
    "$$score(s_{t'},h_{t})= v^{T}tanh(W_1s_{t'}+W_2h_{t})$$\n",
    "\n",
    "$$\\alpha_{tt'} = \\frac{exp(score(h_{t},s_{t'}))}{\\sum_{t''}exp(score(h_{t},s_{t''}))}$$\n",
    "\n",
    "其中$v$，$W_1$，$W_2$都是需要学习的参数，事实上可以使用Dense Layer表示矩阵相乘，并且最后权重计算可以使用Softmax输出。\n",
    "\n",
    "最后得到的参与解码层$t'$时刻的输入的Attention向量是：\n",
    "\n",
    "$$c_{t'} = \\sum_t \\alpha_{tt'}h_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2542,
     "status": "ok",
     "timestamp": 1597645103580,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "O0Zp71mwC5yq"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        #使用Dense layer来表示公式中的矩阵乘法， \n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "        #事实上，units是神经元数量，也代表公式中向量v的长度\n",
    "\n",
    "    def call(self, dec_state, enc_output):\n",
    "        \"\"\"\n",
    "        dec_state: 解码器某个时间的隐层状态，是二维的张量（批量大小，隐藏层大小）\n",
    "        enc_output: 编码器的全部时间的输出，是三维的张量（批量大小，序列长度，隐藏层大小）\n",
    "        \"\"\"\n",
    "        # hidden_with_time_axis 的形状 == （批大小，1，隐藏层大小）\n",
    "        # 这样做是为了方便在加法中使用广播机制以计算分数score\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_state, 1)\n",
    "\n",
    "        # 最后得到的分数score的形状 == （批大小，编码器序列长度，1）\n",
    "        # 我们在最后一个轴上得到 1， 因为我们把分数应用于 self.V\n",
    "        # 在应用 self.V 之前，张量的形状是（批大小，编码器序列长度，Dense Layer大小），这里用了广播机制\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # 注意力权重 （attention_weights） 的形状 == （批大小，编码器序列长度，1）最后这一维向量便是编码器每个时间步对应的权重，是标量\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # 上下文向量 （context_vector） 求和之后的形状 == （批大小，编码器隐藏层大小）\n",
    "        # 这里乘法也在最后一维上使用了广播机制\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bwr-MqrC5yu"
   },
   "source": [
    "使用样例进行测试，得到输出的形状与代码中分析的一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4553,
     "status": "ok",
     "timestamp": 1597645108846,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "7NFNsevPC5yw",
    "outputId": "eadfc34a-42a4-423c-eae6-4f8c1a956eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上下文向量的形状:  (64, 512)   (batch size, units)\n",
      "注意力权重的形状:  (64, 12, 1) (batch_size, sequence_length, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10) # 10是公式中向量v的大小\n",
    "attention_result, attention_weights = attention_layer(sample_state, sample_output)\n",
    "# sample_state事实上是解码器的初始状态，可以作为Attention的第一个输入\n",
    "\n",
    "print(\"上下文向量的形状:  {}   (batch size, units)\".format(attention_result.shape))\n",
    "print(\"注意力权重的形状:  {} (batch_size, sequence_length, 1)\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTFNJlm1C5yz"
   },
   "source": [
    "## 3.3 构造解码器\n",
    "<a id=3.3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNK-Y4weC5yz"
   },
   "source": [
    "我们直接将编码器在最终时间步的隐藏状态作为解码器的初始隐藏状态。这要求编码器和解码器的循环神经网络使用相同的隐藏层个数和隐藏单元个数。\n",
    "\n",
    "在解码器的前向计算中，通过刚刚构造的注意力机制计算得到当前时间步的上下文向量。由于解码器的输入来自输出语言的文本向量，我们将输入通过词嵌入层得到低维的文本向量，然后和上下文向量（context_vector）在特征维连结。我们将连结后的结果与上一时间步的隐藏状态通过门控循环单元计算出当前时间步的输出与隐藏状态。最后，我们将输出通过全连接层变换为有关各个输出词的预测。\n",
    "\n",
    "需要注意的是，我们训练的时候，并不是一次性把整个输出文本的序列输入到解码器，而是逐个输入到解码器，得到输出后在输入到下一个解码器，因此每个解码器GRU的序列长度为1，输入文本的形状为（批量大小，1），但是通过多层解码器的叠加，也实现了文本序列长度的GRU模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2195,
     "status": "ok",
     "timestamp": 1597645171450,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "MTv-8COVC5y0"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "        def __init__(self, vocab_dim, embedding_dim, dec_units, batch_sz):\n",
    "            \"\"\"\n",
    "            vocab_dim： 中文文本词典的维度\n",
    "            embedding_dim： 同编码器，嵌入层使用这两个维度来学习词向量\n",
    "            dec_units: 解码器GRU层神经元数量\n",
    "            batch_sz:  批量大小\n",
    "            \"\"\"\n",
    "            super(Decoder, self).__init__()\n",
    "            self.batch_sz = batch_sz\n",
    "            self.dec_units = dec_units\n",
    "            self.embedding = Embedding(vocab_dim, embedding_dim)\n",
    "            self.gru = GRU(self.dec_units,\n",
    "                            return_sequences=True,\n",
    "                            return_state=True,\n",
    "                            recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "            self.fc = Dense(vocab_dim)\n",
    "            #使用注意力机制\n",
    "            self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        def call(self, x, hidden, enc_output):\n",
    "            \"\"\"\n",
    "            x: 批量数据， 输入形状为（批量大小，1），时间步为1，因为解码需要一个一个输出\n",
    "            hidden: 这里输入上一个解码器的状态，是二维张量（批量大小，解码器隐藏层大小）\n",
    "            enc_output: 编码器的输出（attention用），是三维张量（批量大小，编码器序列长度，编码器隐藏层大小）\n",
    "            \"\"\"\n",
    "            # 得到上下文向量和注意力权重\n",
    "            context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "            # 词嵌入\n",
    "            x = self.embedding(x)\n",
    "            # 将文本数据的向量与上下文向量一起输入GRU层，这时输入向量的维度为（批量大小，1， 嵌入维度 + 编码器隐藏层大小）\n",
    "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "            # 得到gru的输出，这里由于输入的序列是1，所以output的形状为（批量大小，1，隐藏层大小）\n",
    "            # state的形状为（批量大小，隐藏层大小），这里需要返回state作为下一个attention的输入\n",
    "            output, state = self.gru(x)\n",
    "            # 将时间步的维度去掉，然后送入全连接层进行预测\n",
    "            output = tf.reshape(output, (-1, output.shape[2]))\n",
    "            x = self.fc(output)\n",
    "\n",
    "            return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Y1F3yKyC5y4"
   },
   "source": [
    "测试样例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1597645182720,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "LyMH0sG6C5y4",
    "outputId": "50f5c05d-d534-4564-a20c-b576e3f55e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解码器输出形状: (64, 7082) (批量大小, 中文字典大小)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(target_dim, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_state, sample_output)\n",
    "\n",
    "print ('解码器输出形状: {} (批量大小, 中文字典大小)'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjGgxdDtC5y-"
   },
   "source": [
    "# 4. 训练模型\n",
    "<a id=4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PmPHDKK0C5y_"
   },
   "source": [
    "## 4.1 定义优化方法和损失函数\n",
    "<a id=4.1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1681,
     "status": "ok",
     "timestamp": 1597645195829,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "wgfE98AXC5zA"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    #真实值中等于0的为false，不等于0的为True\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    #文本padding后的0不参与计算损失\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1627,
     "status": "ok",
     "timestamp": 1597645198490,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "sZ1nVLClC5zD"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WASq_QCVC5zG"
   },
   "source": [
    "## 4.2 定义训练步骤\n",
    "<a id=4.2></a>\n",
    "\n",
    "1. 将文本输入向量传送至编码器，编码器返回每个时间步的输出和编码器最后一步的隐藏层状态。\n",
    "2. 将编码器输出、编码器隐藏层状态和解码器输入（即开始标记）传送至解码器。\n",
    "3. 解码器返回预测和解码器隐藏层状态。\n",
    "4. 解码器隐藏层状态被传送回模型，预测被用于计算损失。\n",
    "5. 使用教师强制 （teacher forcing） 决定解码器的下一个输入。\n",
    "6. 教师强制指的是将真实的输出文本向量中的下一个词向量作为下一个输入传送至解码器。\n",
    "7. 最后一步是计算梯度，并将其应用于优化器和反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1717,
     "status": "ok",
     "timestamp": 1597645202599,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "nVlJI7IWC5zG"
   },
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    \"\"\"\n",
    "    inp : 输入文本向量，为二维张量（批量大小，序列长度）\n",
    "    targ： 输出文本向量，为二维张量（批量大小，序列长度）\n",
    "    enc_hidden： 编码器初始状态\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 将输入文本向量传送至编码器，编码器返回编码器每个时间步的输出和编码器最后一步的隐藏层状态\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        # 将编码器的最后一步隐层状态作为输入解码器的初始隐层状态\n",
    "        dec_hidden = enc_hidden\n",
    "        # 标识符<start>作为解码器第一个输入的文本向量\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # 教师强制 - 将真实的输出文本向量（targ）中的下一个词向量作为下一个输入\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # 将编码器输出 （enc_output） 上一个解码器的隐层状态（dec_hidden）传送至解码器\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            # 计算loss\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # 使用教师强制\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    #每一批次的损失\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    #训练变量\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    #计算梯度\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    #使用Adam优化算法计算梯度\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phN_eCqaC5zK"
   },
   "source": [
    "## 4.3 训练模型\n",
    "<a id=4.3></a>\n",
    "使用定义好的训练函数训练数据，这里实际运行了50个epochs，但只显示最后10个epochs达到的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 315197,
     "status": "ok",
     "timestamp": 1597650017828,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "BfnRnIydC5zK",
    "outputId": "175c8846-9b94-4489-c9fe-bbe1537d693d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10  Time: 31s  Loss: 0.0723\n",
      "  \n",
      "Epoch: 2/10  Time: 31s  Loss: 0.0653\n",
      "  \n",
      "Epoch: 3/10  Time: 31s  Loss: 0.0626\n",
      "  \n",
      "Epoch: 4/10  Time: 31s  Loss: 0.0552\n",
      "  \n",
      "Epoch: 5/10  Time: 31s  Loss: 0.0450\n",
      "  \n",
      "Epoch: 6/10  Time: 32s  Loss: 0.0377\n",
      "  \n",
      "Epoch: 7/10  Time: 31s  Loss: 0.0341\n",
      "  \n",
      "Epoch: 8/10  Time: 31s  Loss: 0.0297\n",
      "  \n",
      "Epoch: 9/10  Time: 32s  Loss: 0.0272\n",
      "  \n",
      "Epoch: 10/10  Time: 31s  Loss: 0.0271\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # 从创建好的dataset中抽取最多steps_per_epoch个数据\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    # 每 2 个周期（epoch），保存（检查点）一次模型\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch: {}/{}  Time: {:.0f}s  Loss: {:.4f}\\n  '.format(epoch + 1, EPOCHS, time.time() - start,\n",
    "                          total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lTZBnqzC5zN"
   },
   "source": [
    "# 5. 翻译句子\n",
    "<a id=5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgBFVMx3W3D9"
   },
   "source": [
    "翻译句子的步骤与训练时的步骤基本一致，只是要重新对句子做预处理，而且不使用教学机制，即以解码器的输出作为下一个解码器的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4189,
     "status": "ok",
     "timestamp": 1597650088303,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "A-Mq6E18UJzb"
   },
   "outputs": [],
   "source": [
    "def translate2chinese(sentence):\n",
    "    \"\"\"\n",
    "    sentence: 输入一个英文句子字符串\n",
    "    \"\"\"\n",
    "    # 将输入的英文句子作同样的预处理，转换成整数序列\n",
    "    sentence = preprocess_engtext(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = pad_sequences([inputs], maxlen=input_data.shape[1], padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    # 输出结果\n",
    "    result = ''\n",
    "    # 输入到编码器\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
    "    # 输入到解码器，这里不使用教学机制\n",
    "    for t in range(target_data.shape[1]):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # 得到预测值\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        # 根据预测值生成文本\n",
    "        result += target_tokenizer.index_word[predicted_id] + ' '\n",
    "        # 若预测出结束标识符，停止进行下一步预测\n",
    "        if target_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "        # 预测的 ID 被输送回模型\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2968,
     "status": "ok",
     "timestamp": 1597650180536,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "n6VJozm7C5zY",
    "outputId": "9a1772b5-a3a4-4647-d902-2c7d0ed881ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fef7265a7f0>"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3777,
     "status": "ok",
     "timestamp": 1597650249836,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "GAYBJcaZC5zd",
    "outputId": "8614f81e-3861-49aa-df46-74ca9293bbf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> have a nice day . <end>\n",
      "Translation: 祝 你 一天 过得 愉快 。 <end> \n"
     ]
    }
   ],
   "source": [
    "translate2chinese(\"Have a nice day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1597650690781,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "bH7H2CPkYBgN",
    "outputId": "f8c13475-a514-43b9-ef0a-6b3c61a35649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i'll wait until four o'clock . <end>\n",
      "Translation: 我會 等到 四點 。 <end> \n"
     ]
    }
   ],
   "source": [
    "translate2chinese(\"I'll wait until four o'clock.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1597650736606,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "1YVOWbiNYG0V",
    "outputId": "2f1d8506-b0f7-4da9-9bc6-64a4e8c86e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i want to become an engineer . <end>\n",
      "Translation: 我 想要 成為 一位 工程 師 。 <end> \n"
     ]
    }
   ],
   "source": [
    "translate2chinese(\"I want to become an engineer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prwjfb7faZoh"
   },
   "source": [
    "也有一些句子翻译的还不好。这种句子并不是从单词推断意思，而是这整个句子是一个意思。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2274,
     "status": "ok",
     "timestamp": 1597650947017,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "NrVQpJHtaaY4",
    "outputId": "a6a86be4-f6c9-4b9b-bd3a-c8019c38073d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> how are you . <end>\n",
      "Translation: 你 呢 嗎 ？ <end> \n"
     ]
    }
   ],
   "source": [
    "translate2chinese(\"How are you.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zzUYFVoaPnS"
   },
   "source": [
    "由于数据只是一些简单的中英文本对，加之训练的数据非常少，并不能翻译稍复杂的句子，而且对于训练数据的翻译效果比较好，泛化性能却很差，但是这个模型还是可以自动生成比较通顺的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1136,
     "status": "ok",
     "timestamp": 1597651533117,
     "user": {
      "displayName": "yu song",
      "photoUrl": "",
      "userId": "12746579524789329576"
     },
     "user_tz": -480
    },
    "id": "TeycOMsAYdAQ",
    "outputId": "793051c1-3cb2-4aea-f61a-d0c17b57aec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i would rather live by myself than do as he tells me to do . <end>\n",
      "Translation: 我 非常 比 任何人 都 喜歡 網球 。 <end> \n"
     ]
    }
   ],
   "source": [
    "translate2chinese(\"I would rather live by myself than do as he tells me to do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fC7Lv4yudGFf"
   },
   "source": [
    "诶，想可视化Attention机制也失败了，总之，结果不太理想，还是有很多细节需要修改啊！！！"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "机器翻译——英译汉.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
